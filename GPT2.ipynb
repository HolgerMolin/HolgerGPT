{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HolgerMolin/HolgerGPT/blob/main/GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cqdQNipKZLyS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y9nHrffNZrJP"
   },
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "  def __init__(self, embedding_dim, hidden_dim):\n",
    "    super().__init__()\n",
    "    self.linear_1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "    self.linear_2 = nn.Linear(hidden_dim, embedding_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.linear_2(F.relu(self.linear_1(x)))\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, key_dim, context_length):\n",
    "        super().__init__()\n",
    "        self.w_q = torch.randn((embedding_dim, key_dim), requires_grad=True)\n",
    "        self.w_k = torch.randn((embedding_dim, key_dim), requires_grad=True)\n",
    "        self.w_v1 = torch.randn((embedding_dim, key_dim), requires_grad=True)\n",
    "        self.w_v2 = torch.randn((key_dim, embedding_dim), requires_grad=True)\n",
    "\n",
    "        self.mask = torch.triu(torch.ones(context_length, context_length)) == 0\n",
    "        self.k_dim = key_dim\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.w_k\n",
    "        queries = x @ self.w_q\n",
    "        attention = queries @ torch.transpose(keys, 1, 2) * (self.k_dim ** -0.5)\n",
    "        attention = attention.masked_fill(self.mask, -float('inf'))\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "        values = x @ self.w_v1 @ self.w_v2\n",
    "        additions = attention @ values\n",
    "\n",
    "        return additions\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, embedding_dim, num_heads, context_length):\n",
    "    super().__init__()\n",
    "    assert embedding_dim % num_heads == 0\n",
    "    self.key_dim = int(embedding_dim / num_heads)\n",
    "    self.heads = nn.ModuleList(AttentionHead(embedding_dim, self.key_dim, context_length) for _ in range(num_heads))\n",
    "\n",
    "  def forward(self, x):\n",
    "    for head in self.heads:\n",
    "        x += head(x)\n",
    "    return x\n",
    "      \n",
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, embedding_dim, num_heads, hidden_dim, context_length):\n",
    "    super().__init__()\n",
    "    self.ffn = FFN(embedding_dim, hidden_dim)\n",
    "    self.attention = MultiHeadAttention(embedding_dim, num_heads, context_length)\n",
    "    self.norm1 = nn.LayerNorm((context_length, embedding_dim))\n",
    "    self.norm2 = nn.LayerNorm((context_length, embedding_dim))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x += self.ffn(x)\n",
    "    x = self.norm1(x)\n",
    "    x += self.attention(x)\n",
    "    x = self.norm2(x)\n",
    "    return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, context_length, embedding_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.context_length = context_length\n",
    "\n",
    "        pe = torch.zeros(context_length, embedding_dim)\n",
    "        position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "class HolgerGPT(nn.Module):\n",
    "  def __init__(self, num_layers, embedding_dim, num_heads, hidden_dim, context_length, vocab_size):\n",
    "    super().__init__()\n",
    "    self.word_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    self.pos_embedding = PositionalEncoding(context_length, embedding_dim)\n",
    "    self.transformer_blocks = nn.ModuleList(TransformerBlock(embedding_dim, num_heads, hidden_dim, context_length) for _ in range(num_layers))\n",
    "    self.linear = nn.Linear(embedding_dim * context_length, context_length)\n",
    "    self.embedding_dim = embedding_dim\n",
    "    self.context_length = context_length\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.word_embedding(x)\n",
    "    x = self.pos_embedding(x)\n",
    "    for block in self.transformer_blocks:\n",
    "      x = block.forward(x)\n",
    "    x = self.linear(x.reshape(-1, self.embedding_dim * self.context_length))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 9_209_600\n"
     ]
    }
   ],
   "source": [
    "model = HolgerGPT(4, 128, 32, 256, 256, 256)\n",
    "print(f'Number of parameters: {sum(p.numel() for p in model.parameters()):_}')\n",
    "x = torch.ones((128, 256), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyODYauqXomLi5jwEeU5Z0G8",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
